<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fressanlage</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh@latest"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        body { text-align: center; font-family: Arial, sans-serif; }
        video, canvas { position: absolute; left: 50%; transform: translateX(-50%); }
        video { width: 640px; height: 480px; border: 2px solid black; }
        #chartCanvas { position: relative; margin-top: 520px; width: 400px; height: 200px; }
    </style>
</head>
<body>
    <h1>Fressanlage: Mundöffnungserkennung</h1>
    <video id="video" autoplay playsinline></video>
    <canvas id="overlay"></canvas>
    <canvas id="chartCanvas"></canvas>

    <script>
        let openMouthTime = 0, closedMouthTime = 0, lastState = "closed", lastChangeTime = Date.now();
        let video = document.getElementById('video');

        async function setupCamera() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                video.srcObject = stream;
                return new Promise((resolve) => {
                    video.onloadedmetadata = () => {
                        video.play();
                        console.log("Kamera erfolgreich gestartet.");
                        resolve(video);
                    };
                });
            } catch (error) {
                console.error("Fehler beim Starten der Kamera:", error);
            }
        }

        async function detectMouth() {
            const video = await setupCamera();
            await new Promise(resolve => setTimeout(resolve, 1000)); // Sicherheitspuffer

            // Warte, bis das Video eine gültige Größe hat
            while (video.videoWidth === 0 || video.videoHeight === 0) {
                console.log("Warte auf Webcam-Feed...");
                await new Promise(resolve => setTimeout(resolve, 100));
            }

            const model = await facemesh.load();
            const canvas = document.getElementById('overlay');
            const ctx = canvas.getContext('2d');

            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;

            async function analyzeFrame() {
                if (video.videoWidth === 0 || video.videoHeight === 0) {
                    console.warn("Video hat keine gültige Größe. Überspringe Frame.");
                    requestAnimationFrame(analyzeFrame);
                    return;
                }

                ctx.clearRect(0, 0, canvas.width, canvas.height);

                try {
                    const predictions = await model.estimateFaces({ input: video });

                    if (predictions.length > 0) {
                        const keypoints = predictions[0].scaledMesh;
                        const upperLip = keypoints[13], lowerLip = keypoints[14];
                        const mouthOpen = Math.abs(upperLip[1] - lowerLip[1]) > 15;

                        drawMouthOutline([keypoints[61], keypoints[146], keypoints[91], keypoints[181], keypoints[84], keypoints[17]], mouthOpen);

                        let currentTime = Date.now();
                        if (mouthOpen && lastState === "closed") {
                            closedMouthTime += (currentTime - lastChangeTime) / 1000;
                            lastState = "open";
                            lastChangeTime = currentTime;
                        } else if (!mouthOpen && lastState === "open") {
                            openMouthTime += (currentTime - lastChangeTime) / 1000;
                            lastState = "closed";
                            lastChangeTime = currentTime;
                        }
                    }
                } catch (error) {
                    console.error("Fehler in analyzeFrame():", error);
                }

                updateChart();
                requestAnimationFrame(analyzeFrame);
            }

            function drawMouthOutline(mouthLandmarks, isOpen) {
                ctx.beginPath();
                ctx.moveTo(mouthLandmarks[0][0], mouthLandmarks[0][1]);
                for (let i = 1; i < mouthLandmarks.length; i++) {
                    ctx.lineTo(mouthLandmarks[i][0], mouthLandmarks[i][1]);
                }
                ctx.closePath();
                ctx.strokeStyle = isOpen ? 'red' : 'blue';
                ctx.lineWidth = 3;
                ctx.stroke();
            }

            analyzeFrame();
        }

        function updateChart() {
            if (!window.myChart) {
                const ctx = document.getElementById('chartCanvas').getContext('2d');
                window.myChart = new Chart(ctx, {
                    type: 'bar',
                    data: { labels: ['Mund geschlossen', 'Mund offen'], datasets: [{ label: 'Zeit (Sekunden)', data: [closedMouthTime, openMouthTime], backgroundColor: ['blue', 'red'] }] },
                    options: { responsive: false, scales: { y: { beginAtZero: true } } }
                });
            } else {
                window.myChart.data.datasets[0].data = [closedMouthTime, openMouthTime];
                window.myChart.update();
            }
        }

        detectMouth();
    </script>
</body>
</html>
