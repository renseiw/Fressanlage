<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mundöffnungserkennung mit TensorFlow.js</title>
    <!-- TensorFlow.js und Face-Landmarks-Detection laden -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.21.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection"></script>
    <!-- Chart.js -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        video {
            border: 2px solid #333;
            border-radius: 8px;
            width: 320px;
            height: 240px;
            display: block;
            margin-bottom: 1rem;
        }
        canvas {
            max-width: 400px;
            max-height: 200px;
            margin-bottom: 1rem;
        }
        #status {
            margin-bottom: 1rem;
        }
    </style>
</head>
<body>
    <h2>Mundöffnungs-Erkennung mit TensorFlow.js</h2>
    <p id="status">Lade Modell...</p>
    <video id="video" autoplay muted></video>
    <canvas id="outputCanvas" width="320" height="240"></canvas>
    <canvas id="chartCanvas" width="400" height="200"></canvas>

    <script>
        // Ask the user what behavior is expected if multiple faces appear.
        // Currently, the code only processes the first face in view.
        // Is that okay, or do you want to track multiple faces?

        let model;
        let video = document.getElementById('video');
        let outputCanvas = document.getElementById('outputCanvas');
        // Renaming variable to avoid duplicate declarations
        let canvasCtx = outputCanvas.getContext('2d');

        let openTime = 0;
        let closedTime = 0;
        let lastState = 'closed';
        let lastChangeTime = Date.now();

        let chart;

        async function setupCamera() {
            const stream = await navigator.mediaDevices.getUserMedia({ video: true });
            video.srcObject = stream;
            return new Promise((resolve) => {
                video.onloadedmetadata = () => {
                    video.play();
                    resolve();
                };
            });
        }

        function updateChart() {
            if (!chart) {
                const chartCtx = document.getElementById('chartCanvas').getContext('2d');
                chart = new Chart(chartCtx, {
                    type: 'bar',
                    data: {
                        labels: ['Offen', 'Geschlossen'],
                        datasets: [
                            {
                                label: 'Zeit (Sekunden)',
                                data: [openTime, closedTime],
                                backgroundColor: ['red', 'green']
                            }
                        ]
                    },
                    options: {
                        responsive: true,
                        scales: {
                            y: {
                                beginAtZero: true
                            }
                        }
                    }
                });
            } else {
                chart.data.datasets[0].data = [openTime, closedTime];
                chart.update();
            }
        }

        // Einfache Distanzfunktion
        function euclideanDistance(p1, p2) {
            const dx = p1.x - p2.x;
            const dy = p1.y - p2.y;
            return Math.sqrt(dx * dx + dy * dy);
        }

        // Bestimme basierend auf Landmark-Punkten, ob der Mund offen ist
        function isMouthOpen(landmarks) {
            // MediaPipe-Face-Mesh Indizes (ungefähr):
            // Obere Lippe: 13, Untere Lippe: 14
            const upperLip = landmarks[13];
            const lowerLip = landmarks[14];
            const distance = euclideanDistance(
                { x: upperLip.x, y: upperLip.y },
                { x: lowerLip.x, y: lowerLip.y }
            );
            // Einfacher Grenzwert, hier z.B. 0.02
            return distance > 0.02;
        }

        async function detectFace() {
            // Zeichne das Kamerabild auf das Canvas
            canvasCtx.drawImage(video, 0, 0, outputCanvas.width, outputCanvas.height);

            // Führe die Gesichtserkennung aus
            // Sicherstellen, dass das Modell geladen ist
            if (!model) {
                requestAnimationFrame(detectFace);
                return;
            }

            const predictions = await model.estimateFaces({
                input: video,
                returnTensors: false,
                flipHorizontal: false,
                predictIrises: false
            });

            let now = Date.now();
            let state = 'closed';

            if (predictions.length > 0) {
                const keypoints = predictions[0].keypoints;
                if (isMouthOpen(keypoints)) {
                    state = 'open';
                }

                // Landmarks zur Visualisierung
                canvasCtx.strokeStyle = 'lime';
                for (let i = 0; i < keypoints.length; i++) {
                    const x = keypoints[i].x;
                    const y = keypoints[i].y;
                    canvasCtx.beginPath();
                    canvasCtx.arc(x, y, 1, 0, 2 * Math.PI);
                    canvasCtx.stroke();
                }
            }

            // Zustand prüfen
            if (state !== lastState) {
                const duration = (now - lastChangeTime) / 1000;
                if (lastState === 'open') {
                    openTime += duration;
                } else {
                    closedTime += duration;
                }
                lastChangeTime = now;
                lastState = state;
                updateChart();
            }

            requestAnimationFrame(detectFace);
        }

        async function init() {
            document.getElementById('status').innerText = 'Erbitte Kamerazugriff...';
            await setupCamera();
            document.getElementById('status').innerText = 'Lade Modell...';

            // Modell laden
            model = await faceLandmarksDetection.createDetector(
                faceLandmarksDetection.SupportedModels.MediaPipeFaceMesh,
                {
                    runtime: 'tfjs',
                    refineLandmarks: false,
                    maxFaces: 1
                }
            );
            document.getElementById('status').innerText = 'Modell geladen, starte Erkennung...';
            requestAnimationFrame(detectFace);
        }

        init();
    </script>

    <!--
    Test Cases:
    1. Verify that the HTML file loads without syntax errors.
    2. Check that the camera feed displays in the <video> element.
    3. Confirm that the chart updates (Offen/Geschlossen) when mouth states switch.
    4. If mouth stays open for ~3 seconds, ensure \"Offen\" bar in the chart increments by ~3.
    5. If mouth is closed for ~2 seconds, ensure \"Geschlossen\" bar increments by ~2.
    -->
</body>
</html>
