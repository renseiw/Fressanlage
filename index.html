<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mundöffnungserkennung mit TensorFlow.js</title>
    <!-- TensorFlow.js und Face-Landmarks-Detection laden -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.21.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection"></script>
    <!-- Chart.js -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        video {
            border: 2px solid #333;
            border-radius: 8px;
            width: 320px;
            height: 240px;
            display: block;
            margin-bottom: 1rem;
        }
        canvas {
            max-width: 400px;
            max-height: 200px;
            margin-bottom: 1rem;
        }
        #status {
            margin-bottom: 1rem;
        }
    </style>
</head>
<body>
    <h2>Mundöffnungs-Erkennung mit TensorFlow.js</h2>
    <p id="status">Lade Modell...</p>
    <video id="video" autoplay muted></video>
    <canvas id="outputCanvas" width="320" height="240"></canvas>
    <canvas id="chartCanvas" width="400" height="200"></canvas>

    <script>
        // Fragen an den Nutzer:
        // 1. Was soll passieren, wenn mehrere Gesichter erkannt werden?
        //    Wir betrachten standardmäßig nur predictions[0].
        // 2. Wie soll ein Grenzwert wie 0.02 bei "isMouthOpen" behandelt werden?
        //    Offen oder zu?

        // Globale Variablen
        let model = null;
        let video = document.getElementById('video');
        let canvas = document.getElementById('outputCanvas');
        let ctx = canvas.getContext('2d');

        let openTime = 0;
        let closedTime = 0;
        let lastState = 'closed';
        let lastChangeTime = Date.now();

        let chart;

        // Setup der Kamera
        async function setupCamera() {
            const stream = await navigator.mediaDevices.getUserMedia({ video: true });
            video.srcObject = stream;
            return new Promise((resolve) => {
                video.onloadedmetadata = () => {
                    video.play();
                    resolve();
                };
            });
        }

        // Aktualisiert das Balkendiagramm
        function updateChart() {
            if (!chart) {
                const chartCtx = document.getElementById('chartCanvas').getContext('2d');
                chart = new Chart(chartCtx, {
                    type: 'bar',
                    data: {
                        labels: ['Offen', 'Geschlossen'],
                        datasets: [
                            {
                                label: 'Zeit (Sekunden)',
                                data: [openTime, closedTime],
                                backgroundColor: ['red', 'green']
                            }
                        ]
                    },
                    options: {
                        responsive: true,
                        scales: {
                            y: {
                                beginAtZero: true
                            }
                        }
                    }
                });
            } else {
                chart.data.datasets[0].data = [openTime, closedTime];
                chart.update();
            }
        }

        // Einfache Distanzfunktion
        function euclideanDistance(p1, p2) {
            const dx = p1.x - p2.x;
            const dy = p1.y - p2.y;
            return Math.sqrt(dx * dx + dy * dy);
        }

        // Bestimmt basierend auf Landmark-Punkten, ob der Mund offen ist
        // Beispiel: Oberer (13) und unterer (14) mittlerer Lippenpunkt
        function isMouthOpen(landmarks) {
            const upperLip = landmarks[13];
            const lowerLip = landmarks[14];
            const distance = euclideanDistance(
                { x: upperLip.x, y: upperLip.y },
                { x: lowerLip.x, y: lowerLip.y }
            );
            // Einfacher Grenzwert, hier z.B. 0.02
            return distance > 0.02;
        }

        // Hauptschleife der Gesichtserkennung
        async function detectFace() {
            // Sicherstellen, dass das Modell geladen ist
            if (!model) {
                // Warte, bis Modell verfügbar ist, und versuche es erneut
                requestAnimationFrame(detectFace);
                return;
            }

            // Zeichne das Kamerabild auf das Canvas
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

            // Führe die Gesichtserkennung aus (Fehler abfangen)
            let predictions = [];
            try {
                predictions = await model.estimateFaces({
                    input: video,
                    returnTensors: false,
                    flipHorizontal: false,
                    predictIrises: false
                });
            } catch (err) {
                console.error('Fehler bei faceLandmarksDetection:', err);
                requestAnimationFrame(detectFace);
                return;
            }

            const now = Date.now();
            let state = 'closed'; // default

            if (predictions.length > 0) {
                // Nur das erste Gesicht (falls nur eine Person)
                const keypoints = predictions[0].keypoints;

                // Prüfen, ob Mund offen
                if (isMouthOpen(keypoints)) {
                    state = 'open';
                }

                // Landmarks zeichnen
                ctx.strokeStyle = 'lime';
                for (let i = 0; i < keypoints.length; i++) {
                    const x = keypoints[i].x;
                    const y = keypoints[i].y;
                    ctx.beginPath();
                    ctx.arc(x, y, 1, 0, 2 * Math.PI);
                    ctx.stroke();
                }
            }

            // Wenn sich der Zustand ändert, update die Zeit
            if (state !== lastState) {
                const duration = (now - lastChangeTime) / 1000;
                if (lastState === 'open') {
                    openTime += duration;
                } else {
                    closedTime += duration;
                }
                lastChangeTime = now;
                lastState = state;
                updateChart();
            }

            requestAnimationFrame(detectFace);
        }

        // Initialisierung
        async function init() {
            document.getElementById('status').innerText = 'Erbitte Kamerazugriff...';
            await setupCamera();
            document.getElementById('status').innerText = 'Lade Modell...';

            // Modell laden
            try {
                model = await faceLandmarksDetection.createDetector(
                    faceLandmarksDetection.SupportedModels.MediaPipeFaceMesh,
                    {
                        runtime: 'tfjs',
                        refineLandmarks: false,
                        maxFaces: 1
                    }
                );
            } catch (err) {
                console.error('Fehler beim Laden des Modells:', err);
                document.getElementById('status').innerText = 'Fehler beim Modell-Laden';
                return;
            }

            document.getElementById('status').innerText = 'Modell geladen, starte Erkennung...';

            // Bestehende Test Cases für isMouthOpen
            console.log('--- Starte Unit Tests für isMouthOpen ---');
            (function runTestCases() {
                const testLandmarks = [];
                // Test 1: Distanz ca. 0.03 (Mund offen)
                testLandmarks[13] = { x: 0.1, y: 0.1 };
                testLandmarks[14] = { x: 0.1, y: 0.13 };
                console.log('Test 1 (erwartet: true) =>', isMouthOpen(testLandmarks));

                // Test 2: Distanz ca. 0.01 (Mund geschlossen)
                testLandmarks[14] = { x: 0.1, y: 0.11 };
                console.log('Test 2 (erwartet: false) =>', isMouthOpen(testLandmarks));

                // Neuer Test 3: Distanz = 0.02 (Grenzwert) => Fraglich, je nach Definition
                // Hier nehmen wir an: "strictly greater" => false
                testLandmarks[14] = { x: 0.1, y: 0.12 };
                console.log('Test 3 (Distanz=0.02, erwartet: false) =>', isMouthOpen(testLandmarks));
            })();
            console.log('--- Ende Unit Tests ---');

            // Starte Gesichtserkennung
            requestAnimationFrame(detectFace);
        }

        init();
    </script>

    <!--
    Zusätzliche Test Cases:
    1. Testen, ob Fehler geworfen wird, wenn kein Kamerazugriff vorliegt.
    2. Prüfen, ob die Balken (Offen/Geschlossen) korrekt updaten, wenn Mund öfter wechselt.
    -->
</body>
</html>
