<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fressanlage</title>
    <script src="https://docs.opencv.org/4.5.0/opencv.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        body { text-align: center; font-family: Arial, sans-serif; }
        #warning { font-size: 24px; font-weight: bold; color: red; display: none; }
        #chartCanvas { margin-top: 20px; width: 400px; height: 200px; }
    </style>
</head>
<body>
    <h1>Fressanlage</h1>
    <p id="warning">Mach den Mund zu!</p>
    <canvas id="chartCanvas"></canvas>

    <script>
        let openMouthTime = 0, closedMouthTime = 0;
        let lastState = "closed";
        let lastChangeTime = Date.now();
        let video, cap, faceCascade;

        async function setupCamera() {
            video = document.createElement("video");
            video.setAttribute("autoplay", "");
            video.setAttribute("playsinline", "");
            const stream = await navigator.mediaDevices.getUserMedia({ video: true });
            video.srcObject = stream;

            await new Promise(resolve => video.onloadedmetadata = resolve);

            // Warten, bis das Video eine gültige Größe hat
            while (video.videoWidth === 0 || video.videoHeight === 0) {
                console.log("Warte auf Webcam-Feed...");
                await new Promise(resolve => setTimeout(resolve, 100));
            }
            return video;
        }

        async function detectMouth() {
            video = await setupCamera();
            cap = new cv.VideoCapture(video);

            // **Korrektur: Die Frame-Größe wird dynamisch gesetzt!**
            let frame = new cv.Mat(video.videoHeight, video.videoWidth, cv.CV_8UC4);
            let gray = new cv.Mat();
            let faces = new cv.RectVector();

            faceCascade = new cv.CascadeClassifier();
            await new Promise(resolve => {
                cv.FS_createPreloadedFile("/", "haarcascade_frontalface_default.xml",
                    "https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml",
                    true, false, resolve);
            });

            faceCascade.load("haarcascade_frontalface_default.xml");

            async function processFrame() {
                if (video.videoWidth === 0 || video.videoHeight === 0) {
                    console.warn("Video hat keine gültige Größe. Überspringe Frame.");
                    requestAnimationFrame(processFrame);
                    return;
                }

                // **Korrektur: Frame-Größe mit Video synchronisieren**
                if (frame.cols !== video.videoWidth || frame.rows !== video.videoHeight) {
                    frame = new cv.Mat(video.videoHeight, video.videoWidth, cv.CV_8UC4);
                }

                cap.read(frame);
                cv.cvtColor(frame, gray, cv.COLOR_RGBA2GRAY);

                // Gesichtserkennung
                faceCascade.detectMultiScale(gray, faces, 1.1, 3, 0);

                let mouthOpen = false;
                for (let i = 0; i < faces.size(); i++) {
                    let face = faces.get(i);
                    let mouthRegion = gray.roi(new cv.Rect(face.x, face.y + face.height / 2, face.width, face.height / 2));

                    // Berechne Helligkeit im unteren Gesichtsbereich
                    let mean = cv.mean(mouthRegion);
                    mouthOpen = mean[0] > 120; // Falls zu hell, könnte der Mund offen sein

                    mouthRegion.delete();
                }

                // **Zeitmessung für Mundstatus**
                let currentTime = Date.now();
                if (mouthOpen && lastState === "closed") {
                    closedMouthTime += (currentTime - lastChangeTime) / 1000;
                    lastState = "open";
                    lastChangeTime = currentTime;
                    document.getElementById("warning").style.display = "block";
                } else if (!mouthOpen && lastState === "open") {
                    openMouthTime += (currentTime - lastChangeTime) / 1000;
                    lastState = "closed";
                    lastChangeTime = currentTime;
                    document.getElementById("warning").style.display = "none";
                }

                updateChart();
                requestAnimationFrame(processFrame);
            }

            processFrame();
        }

        function updateChart() {
            if (!window.myChart) {
                const ctx = document.getElementById('chartCanvas').getContext('2d');
                window.myChart = new Chart(ctx, {
                    type: 'bar',
                    data: { labels: ['Mund geschlossen', 'Mund offen'], datasets: [{ label: 'Zeit (Sekunden)', data: [closedMouthTime, openMouthTime], backgroundColor: ['blue', 'red'] }] },
                    options: { responsive: false, scales: { y: { beginAtZero: true } } }
                });
            } else {
                window.myChart.data.datasets[0].data = [closedMouthTime, openMouthTime];
                window.myChart.update();
            }
        }

        detectMouth();
    </script>
</body>
</html>
